# -*- coding: utf-8 -*-
"""Fake-a-thon.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1_HGewjCKvAz5x5qoB7_g-AgZPXBp5xjk
"""

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Note:- All the path are relative to MyDrive
from google.colab import drive
drive.mount('/content/drive')

from zipfile import ZipFile
dataset_path = '/content/drive/MyDrive/data.zip'
with ZipFile(dataset_path, 'r') as zip_ref:
    zip_ref.extractall()
  
from keras.models import Sequential
from keras.layers import Conv2D, Flatten, Dense,MaxPool2D
import numpy as np
from keras.preprocessing import image
from keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
# %matplotlib inline

# Data Pre-processing

nbatch = 128
train_data = ImageDataGenerator(rescale=1./255,rotation_range=10.,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.2,horizontal_flip=True)

test_data = ImageDataGenerator(rescale = 1./255)

training_set = train_data.flow_from_directory('/content/data/train', target_size=(128,128),batch_size =nbatch,class_mode = 'binary')
test_set = test_data.flow_from_directory('/content/data/test',target_size=(128,128),batch_size =nbatch,class_mode = 'binary')

# Training Phase

from keras.layers import Input, Flatten, Dense, Dropout, Activation
from tensorflow.keras.layers import LeakyReLU
model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3),input_shape=(128,128,3)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(64, kernel_size=(3, 3)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(128, kernel_size=(3, 3)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(64, kernel_size=(3, 3)))
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Dropout(0.25))
model.add(Flatten())

model.add(Dense(units=256))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(units=128))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(activation="sigmoid",
                units=1))

model.compile(optimizer = 'adam',loss = 'binary_crossentropy',metrics = ['accuracy'])
model.fit_generator(training_set,steps_per_epoch=60,epochs=100,validation_data=test_set,validation_steps=28)

# Testing Phase

import os
import cv2

def ImagePrediction(loc):
    test_image = image.load_img(loc, target_size = (128,128))
    test_image = image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis = 0)
    result = model.predict(test_image)
    return result

pred = []
path = '/content/data/test'
for img in os.listdir(path):
  img_path = os.path.join(path,img)
  result = ImagePrediction(img_path)
  if result[0][0]==1:
    pred.append('real')
  else:
    pred.append('fake')

# Creating Submit.csv file

import pandas as pd
df = pd.read_csv('/content/data/test.csv')
df['label'] = pred
df.to_csv('/content/data/submit.csv', index=False)

# Commented out IPython magic to ensure Python compatibility.
from keras.models import Sequential
from keras.layers import Conv2D, Flatten, Dense,MaxPool2D
import numpy as np
from keras.preprocessing import image
from keras.callbacks import EarlyStopping, ModelCheckpoint
import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
# %matplotlib inline

try:
    from tensorflow.python.util import module_wrapper as deprecation
except ImportError:
    from tensorflow.python.util import deprecation_wrapper as deprecation
deprecation._PER_MODULE_WARNING_LIMIT = 0

nbatch = 128
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10.,
    width_shift_range=0.1,
    height_shift_range=0.1,
    zoom_range=0.2,
    horizontal_flip=True)

test_datagen = ImageDataGenerator(rescale = 1./255)

training_set = train_datagen.flow_from_directory('/content/data/train',
                                                 target_size=(128,128),
                                                 batch_size =nbatch,
                                                 class_mode = 'binary')

test_set = test_datagen.flow_from_directory('/content/data/test',
                                            target_size=(128,128),
                                            batch_size =nbatch,
                                            class_mode = 'binary')

from keras.layers import Input, Flatten, Dense, Dropout, Activation
#from keras.layers.normalization import BatchNormalization
from tensorflow.keras.layers import LeakyReLU
model = Sequential()

model.add(Conv2D(32, kernel_size=(3, 3),input_shape=(128,128,3)))
#model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(64, kernel_size=(3, 3)))
#model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(128, kernel_size=(3, 3)))
#model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Conv2D(64, kernel_size=(3, 3)))
#model.add(BatchNormalization())
model.add(LeakyReLU(alpha=0.1))
model.add(MaxPool2D(pool_size=(2,2)))

model.add(Dropout(0.25))
model.add(Flatten())

model.add(Dense(units=256))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(units=128))
model.add(LeakyReLU(alpha=0.1))
model.add(Dense(activation="sigmoid",
                units=1))

model.summary()

model.compile(optimizer = 'adam',
              loss = 'binary_crossentropy',
              metrics = ['accuracy'])

callbacks_list = [
    EarlyStopping(monitor='val_loss', patience=10),
    ModelCheckpoint(filepath='model_checkpoint.hdf5', monitor='val_loss', save_best_only=True, mode ='max'),
]

history = model.fit_generator(
        training_set,
        steps_per_epoch=60,
        epochs=100,
        validation_data=test_set,
        validation_steps=28,
        callbacks = callbacks_list
    )

import os
import cv2

def ImagePrediction(loc):
    test_image = image.load_img(loc, target_size = (128,128))
    test_image = image.img_to_array(test_image)
    test_image = np.expand_dims(test_image, axis =0)
    result = model.predict(test_image)
    return result

pred = []
cnt = 1
path = '/content/data/test'
for img in os.listdir(path):
  img_path = os.path.join(path,img)
  result = ImagePrediction(img_path)
  if result[0][0]==1:
    pred.append('real')
  else:
    pred.append('fake')
  print(cnt)
  cnt = cnt+1

len(pred)

import pandas as pd
df = pd.read_csv('/content/data/test.csv')
df['label'] = pred
df.to_csv('/content/data/submit.csv', index=False)